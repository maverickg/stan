\documentclass[10pt]{article}
\usepackage{stan-papers}

\title{\Large\bfseries The \code{\bfseries stan::agrad} C++ Automatic
  Differentiation Library}

\author{\large Bob Carpenter \\ {\small Columbia University}
        \\[8pt]
        \large Marcus Brubaker \\ {\small Toyota Technical Institute}
   \and \large Matt Hoffman \\ {\small Adobe Research Labs}
        \\[8pt]
        \large Peter Li \\ {\small Columbia University}
   \and \large Daniel Lee \\ {\small Columbia University}
        \\[8pt]
        \large Michael Betancourt \\ {\small University of Warwick}
}
\date{\vspace*{8pt}\normalsize \today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract} 
  \noindent
  The \code{stan::agrad} C++, reverse-mode automatic differentiation
  library was designed to be usable, extensive and extensibile,
  efficient, scalable, stable, portable, and redistributable.

  Usability is achieved through a simple direct interface and a
  cleanly abstracted functional interface.  The extensive built-in
  library includes functions for matrix operations, linear algebra,
  differential equation solving, and most common probability
  functions.  Extensibility derives from a straightforward
  object-oriented framework for expressions, allowing developers and
  users to easily create custom functions. Efficiency is achieved
  through a combination of custom memory management, subexpression
  caching, traits-based metaprogramming, and expression templates.
  Partial derivatives for compound functions are evaluated lazily for
  improved scalability.  Stability is achieved by taking care with
  arithmetic precision in algebraic expressions and providing stable,
  compound functions where possible. For portability, the library is
  standards-compliant C++ (03) and has been tested for all major
  compilers for Windows, Mac OS X, and Linux.  It is distributed under
  the new BSD license.

  This paper provides an overview of \code{agrad}'s application
  programmer interface (API), examples of its use, and a thorough
  explanation of how it is implemented.  It also demonstrates the
  efficiency and scalability of \code{agrad} by comparing its speed
  and memory usage of gradient calculations to that of several popular
  open-source C++ automatic differentiation systems (Adept, CppAD, and
  Sacado), with results varying dramatically according to the type of
  expression being differentiated.
\end{abstract}

\clearpage
\tableofcontents

\clearpage

\section{Reverse-Mode Automatic Differentiation}

Reverse-mode automatic differentiation operates on an arbitrary
differentiable mathematical function expressed as a computer program
and a specified set of inputs to calculate the derivatives of the
function output with respect to the inputs.  The inputs are
independent variables, the output a dependent variable, and the vector
of derivatives is the gradient evaluated at the independent variables.

For example, automatic differentiation will take a simple C++
expression expression such as \code{x~*~y~/~2} and inputs such as
$\code{x}=6$ and $\code{y}=4$ and produce the output value 12 and pair
of derivatives $(2,3)$ of the result with respect to \code{x} and
\code{y}.  

Reverse-mode automatic differentiation constructs an expression graph
and propagating the chain rule.  In somewhat more detail, it works by
%
\begin{enumerate}
\item converting the formula to a rooted, directed, acyclic expression graph,
  where parent nodes represent function values and child nodes
  represent operands to the function,
\item labeling each node with its value and each edge with 
  the partial derivative of the parent node with respect to the child
  node, 
\item setting the root node's adjoint  to 1 and all other nodes' adjoints
  to 0, 
\item traversing the expression graph top down, visiting each node
  before any of its child nodes, and for each node and each of its
  child nodes, incrementing the child node's adjoint by the product of
  the parent node's adjoint and the partial derivative of the parent
  with respect to the child,
\item noticing that the adjoints hold the derivative of the root node
  with respect to each node, and
\item reading the gradient from the input nodes' adjoints.
\end{enumerate}
%

\subsection{Mechanics of Reverse Mode Automatic Differentiation}

As an example, consider the normal log probability density function
for a variable $y$ with a normal distribution with mean $\mu$ and
standard deviation $\sigma$,
%
\renewcommand{\theequation}{\arabic{equation}}
\begin{equation}\label{normal-log-density.equation}
\log \left( \distro{Normal}(y|\mu,\sigma) \right)
= -\frac{1}{2} \left( \frac{y - \mu}{\sigma} \right)^2
- \log \sigma
- \frac{1}{2} \log (2 \pi)
\end{equation}
%
and its gradient evaluated a point
%
\begin{equation}\label{gradient-normal-log-density.equation}
  \nabla_{\! [y \, \mu \, \sigma]} \log \distro{Normal}(y|\mu,\sigma) 
  = 
  \left[
    -(y - \mu) \sigma^{-2} \ \ \ \
    (y - \mu) \sigma^{-2} \ \ \ \ 
    (y - \mu)^2 \sigma^{-3} - \sigma^{-1}
  \right]
\end{equation}


\newcommand{\gmnode}[3]{\put(#1,#2){\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmnodeobs}[3]{\put(#1,#2){\color{yellow}\circle*{20}}\put(#1,#2){\color{black}\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmnoderoot}[3]{\put(#1,#2){\color{red}\circle*{20}}\put(#1,#2){\color{black}\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmdata}[3]{\put(#1,#2){\makebox(16,16){\footnotesize $#3$}}}
\begin{figure}
\begin{center}
\begin{picture}(200,200)
\gmnoderoot{130}{200}{-}
\put(142,200){\mbox{\color{blue}{\footnotesize $v_{10}$}}}
\gmnode{100}{170}{-}
\put(112,170){\mbox{\color{blue}{\footnotesize $v_{9}$}}}
\gmdata{150}{160}{\mbox{\color{gray}{$0.5\log 2 \pi$}}}
\gmnode{70}{140}{*}
\put(82,140){\mbox{\color{blue}{\footnotesize $v_{7}$}}}
\gmdata{30}{100}{\mbox{\color{gray}{$-.5$}}}
\gmnode{100}{110}{\mbox{\footnotesize pow}}
\put(112,110){\mbox{\color{blue}{\footnotesize $v_{6}$}}}
\gmdata{120}{70}{\mbox{\color{gray}{$2$}}}
\gmnode{190}{80}{\mbox{\footnotesize $\log$}}
\put(202,80){\mbox{\color{blue}{\footnotesize $v_{8}$}}}
\gmnode{70}{80}{/}
\put(82,80){\mbox{\color{blue}{\footnotesize $v_{5}$}}}
\gmnode{40}{50}{-}
\put(52,50){\mbox{\color{blue}{\footnotesize $v_{4}$}}}
\gmnodeobs{10}{20}{\mbox{\footnotesize $y$}}
\put(22,20){\mbox{\color{blue}{\footnotesize $v_{1}$}}}
\gmnodeobs{70}{20}{\mbox{\footnotesize $\mu$}}
\put(82,20){\mbox{\color{blue}{\footnotesize $v_{2}$}}}
\gmnodeobs{130}{20}{\mbox{\footnotesize $\sigma$}}
\put(142,20){\mbox{\color{blue}{\footnotesize $v_{3}$}}}
%
\put(123,193){\vector(-1,-1){16}}
\put(137,193){\color{gray}{\vector(1,-1){16}}}
\put(93,163){\vector(-1,-1){16}}
\put(107,163){\vector(1,-1){76}}
\put(63,133){\color{gray}{\vector(-1,-1){16}}}
\put(77,133){\vector(1,-1){16}}
\put(93,103){{\vector(-1,-1){16}}}
\put(107,103){\color{gray}{\vector(1,-1){16}}}
\put(63,73){\vector(-1,-1){16}}
\put(77,73){\vector(1,-1){46}}
\put(183,73){\vector(-1,-1){46}}
\put(33,43){\vector(-1,-1){16}}
\put(47,43){\vector(1,-1){16}}
\end{picture}
\end{center}
\mycaption{expression-graph}{Expression graph for the normal log
  density function given in \refeq{normal-log-density}.  Each circle
  corresponds to an automatic differentiation variable, with the
  variable name given to the right in blue.  The independent variables
  are highlighted in yellow on the bottom row, with the dependent
  varaible highlighted in red on the top of the graph.  The function
  producing each node is displayed inside the circle, with operands
  denoted by arrows.  Constants are shown in gray with gray arrows
  leading to them because derivatives need not be propagated to
  constant operands.}
\end{figure}
%
This mathematical formula corresponds to the expression graph in
\reffigure{expression-graph}.  Each subexpression corresponds to a
node in the graph, and each edge connects the node representing a
function evaluation to its operands.  
%
\begin{figure}
\begin{center}
\begin{tabular}{c||c|cc}
{\it var} & {\it value} & \multicolumn{2}{|c}{\it partials}
\\ \hline \hline
$v_1$ & $y$ 
\\[2pt]
$v_2$ & $\mu$
\\[2pt]
$v_3$ & $\sigma$
\\[2pt]
$v_4$ & $v_1 - v_2$ & $\partial v_4 \partial v_1 = 1$ 
                   & $\partial v_4 / \partial v_2 = -1$
\\[4pt]
$v_5$ & $v_4 / v_3$ & $\partial v_5 / \partial v_4 = 1/v_3$
                    & $\partial v_5 / \partial v_3 = -v_4 v_3^{-2}$
\\[4pt]
$v_6$ & $\left(v_5\right)^2$
      & \multicolumn{2}{c}{$\partial v_6 / \partial v_5 = 2 v_5$}
\\[4pt]
$v_7$ & $(-0.5) v_6$ & \multicolumn{2}{c}{$\partial v_7 / \partial v_6
                                          = -0.5$}
\\[4pt]
$v_8$ & $\log v_3$ & \multicolumn{2}{c}{$\partial v_8 / \partial v_3 = 1/v_3$}
\\[4pt]
$v_9$ & $v_7 - v_8$ & $\partial v_9 / \partial v_7 = 1$
                    & $\partial v_9 / \partial v_8 = -1$
\\[4pt]
$v_{10}$ & $v_9 - (0.5 \log 2\pi)$ 
         & \multicolumn{2}{c}{$\partial v_{10} / \partial v_9 = 1$}
\end{tabular}
\end{center}
\mycaption{forward-pass}{Example of gradient calculation for the the log
  density function of a normally distributed variable, as defined in
  \refeq{normal-log-density}.  In the forward pass, as the stack is
  constructed from the first input $v_1$ up to the final output
  $v_{10}$, the values of the variables and partials are computed
  numerically according to the formulas given in the value and
  partials columns of the table.}
\end{figure}

\reffigure{forward-pass} illustrates the forward pass used by
reverse-mode automatic differentiation to construct the expression
graph for a program.  The expression graph is constructed in the
ordinary evaluation order, with each subexpression being numbered and
placed on a stack.  The stack is initialized here with the dependent
variables, but this is not required.  Each operand to an expression is
evaluated before the expression node is created and placed on the
stack.  As a result, the stack provides a topological sort of the
nodes in the graph (i.e., a sort in which each node representing an
expression occurs above its subexpression nodes in the stack---see
\citep[Section~2.2.3]{knuth:97}).  \reffigure{forward-pass} lists in
the right column for each node, the partial derivative of the function
represented by the node with respect to its operands.  In
\code{stan::agrad}, most of these partials are evaluated lazily during
the reverse pass based on function's value and its operands' values.


\begin{figure}
\[
\begin{array}{rcl|l}
{\it var} & {\it operation} & {\it value} & {\it result}
\\ \hline \hline
a_{1:9} & = & 0 & a_{1:9} = 0
\\
a_{10} & = & 1 & a_{10} = 1
\\ \hline
a_{9} & {+}{=} & a_{10} \times (1) & a_9 = 1
\\
a_{7} & {+}{=} & a_9 \times (1) & a_7 = 1
\\
a_{8} & {+}{=} & a_9 \times (-1) & a_8 = -1
\\
a_{3} & {+}{=} & a_8 \times (1 / v_3) & a_3 = -1 / v_3
\\
a_{6} & {+}{=} & a_7 \times (-0.5) & a_6 = -0.5
\\
a_{5} & {+}{=} & a_6 \times (2 v_5) & a_5 = -v_5
\\
a_{4} & {+}{=} & a_5 \times (1 / v_3) & a_4 = -v_5 / v_3
\\
a_{3} & {+}{=} & a_5 \times (-v_4 v_3^{-2}) & a_3 = -1 / v_3 + v_5 v_4 v_3^{-2}
\\
a_{1} & {+}{=} & a_4 \times (1) & a_1 = -v_5 / v_3
\\
a_{2} & {+}{=} & a_4 \times (-1) & a_2 = v_5 / v_3
\end{array}
\]
\mycaption{autodiff-stack}{In the reverse pass, the stack is
  traversed from the final output down to the inputs, and as each
  variable is visited, each of its operands is updated with the
  variable's adjoint times the partial with respect to the operand.
  After the reverse pass finishes, $(a_1,a_2,a_3)$ is the gradient of
  the density function evaluated at $(y,\mu,\sigma)$, which matches
  the correct result given in \refeq{gradient-normal-log-density}
  after substitution for $v_4$ and $v_5$.}
\end{figure}
%
\reffigure{autodiff-stack} shows the processing for reverse mode,
which involves an adjoint value for each node.  The adjoints for all
nodes other than the root are initialized to 0; the root's adjoint is
initialized to 1, becuase $\partial x / \partial x = 1$.  The backward
sweep simply walks down the expression stack, and for each node,
propagates derivatives from it down to its operands using the chain
rule.  Because the nodes are in topological order, by the time a node
is visted, its adjoint will represent the partial derivative of the
root of the overall expression with respect to the expression
represented by the node.  Each node then propagates its derivatives to
its operands by incrementing its operands' adjoints by the product of
the expression node's adjoint times the partial with respect to the
operand.  Thus when the reverse pass is completed, the adjoints of the
independent variables hold the gradient of the dependent variable
(function value) with respect to the independent variables (inputs).

\subsection{Comparison to Alternative Methods of Computing Gradients}

\subsubsection{Finite Differencing}

Finite differences is a method to approximate derivatives numerically
rather than symbolically.  Given a positive difference $\epsilon > 0$,
an approximate derivative can be calculated as
\[
\frac{\partial}{\partial x_n} f(x) 
\approx 
\frac{f(x_1,\ldots,x_n + \epsilon, \ldots, x_N) - f(x_1, \ldots, x_N)}
     {\epsilon}
\]
or a bit more accurately with a centered interval as
\[
\frac{\partial}{\partial x_n} f(x) 
\approx 
\frac{f(x_1,\ldots,x_n + \epsilon/2, \ldots, x_N) 
      - f(x_1,\ldots,x_n - \epsilon/2, \ldots, x_N)}
     {\epsilon}.
\]
%
Although straightforward and general, calculating gradients using
finite differences is slow and imprecise.  Finite differencing is slow
because it requires $N + 1$ function evaluations ($2N + 1$ in the
centered case) to compute the gradient of an $N$-ary function.  The
numerical issues with finite differencing arise because a small
$\epsilon$ is required for a good approximation of the derivative, but
a large $\epsilon$ is required for floating-point precision.  Small
$\epsilon$ values are problematic for accuracy because because
subtraction of differently scaled numbers loses a degree of precision
equal to their difference in scales; for example, subtracting a number
on the scale of $10^{-6}$ from a number on the scale of 1 loses 6
orders of precision in the $10^{-6}$-scaled term \citep{higham:2002}.
Thus in practice, the arithmetic precision of finite differencing is
usually at best $10^{-7}$ rather than the maximum of roughly
$10^{-14}$ possible with double-precision floating point calculations.

\subsubsection{Symbolic Differentiation}

Unlike symbolic computer algebra systems, such as Mathematica
\citep{mathematica:2014} or SymPy \citep{sympy:2014}, automatic
differentiation does not explicitly generate a mathematical expression
corresponding to the derivative calculation.  Rather, it calculates
the gradient analytically by applying the chain rule directly to the
original function's subexpressions.





\section{Calculating Gradients and Jacobians with \code{\bfseries
    stan::agrad}}

Reverse-mode automatic differentiation in \code{stan::agrad} can be
used to evaluate gradients of functions from $\reals^N$ to $\reals$ or
Jacobians of differentiable functions from $\reals^N$ to $\reals^M$,
returning values for a specified input point $x \in \reals^N$; see
\refsection{derivative-definitions} for precise definitions of
gradients and Jacobians and their evaluation at a point.

\subsection{Direct Calculation of Gradients of Programs}

The following complete C++ program calculates derivatives of the
function given in \refeq{normal-log-density} with respect to its mean
($\mu$) and standard deviation ($\sigma$) parameters for a constant
outcome ($y$).  The first block assigns the constant and independent
variables, the second block computes the dependent variable and prints
the value, and the final block computes and prints the gradients.
%
\begin{quote}
\begin{Verbatim}
#include <cmath>
#include <boost/math/constants.hpp>
#include <stan/agrad/rev.hpp>

int main() { 
  double y = 1.3;
  stan::agrad::var mu = 0.5, sigma = 1.2;

  stan::agrad::var lp = 0;
  lp -= pow(2 * boost::math::constants::pi<double>(), -0.5);
  lp -= log(sigma);
  lp -= 0.5 * pow((y - mu) / sigma), 2);
  std::cout << "f(mu,sigma)= " << lp.val() << std::endl;

  lp.grad();
  std::cout << " d.f / d.mu = " << mu.adj()
            << " d.f / d.sigma = " << sigma.adj() << std::endl;
}
\end{Verbatim}
\end{quote}
%
Constants like \code{y} are assigned to type \code{double} variables
and independent variables like \code{mu} and \code{sigma} are assigned
to type \code{var}.  The result \code{lp} is assigned type \code{var}
and calculated using ordinary C++ operations involving operators
(e.g., \code{*}, \code{/}), compound assignments (e.g., \code{-=}),
and library functions (e.g., \code{pow}, \code{log}, \code{pi}).  The
value is available through the method (member function) \code{val()} as soon as
operations have been applied.  The call to the method
\code{grad()} propagates derivatives from the dependent variable
\code{lp} down through the expression graph to the independent
variables.  The derivatives of the dependent variable with respect to
the independent variables may then be extracted from the independent
variables with the method \code{adj()}.

The gradient evaluated at the input can also be extracted as a
standard vector using the method \code{grad()} of \code{agrad::var}.
%
\begin{quote}
\begin{Verbatim}
#include <vector>
...
  std::vector<stan::agrad::var> theta;
  theta.push_back(mu);   theta.push_back(sigma);
  std::vector<double> g;
  lp.grad(theta,g);
  std::cout << " d.f / d.mu = " << g[0]
            << " d.f / d.sigma = " << g[1] << std::endl;
\end{Verbatim}
\end{quote}
%
The ellision (\code{...}) represents all of the code from the previous
program up to but not including the final block.  The standard vector
\code{theta} holds the dependent variables and the standard vector
\code{g} is used to hold the result.  The function \code{grad()}
function is called on the dependent variable to propagate the
derivatives and fill \code{g} with the gradient.

\subsection{Coding Template Functions for Automatic Differentiation}

The previous example was implemented directly in the main function
block using primitive operations.  The \code{stan::agrad} library
implements all of the built-in C++ boolean and arithmetic operators
as well as the assignment and compound assignment operators.  
It also implements all of the library functions.  Examples of
operator, assignment, and function implementations are given later.

From a user perspective, a function can be automatically
differentiated with respect to some input parameters if the arguments
to be differentiated can all be instantiated to \code{agrad::var}
types.  For the most flexiblity, functions should be separately
templated in all of their arguments so as to support any combination
of primitive (e.g., \code{double}, \code{int}) and autodiff
(\code{agrad::var}) instantiations.

For example, the following templated C++ function computes the log
normal density as defined in \refeq{normal-log-density}.
%%
\begin{quote}
\begin{Verbatim}
#include <boost/math/tools/promotion.hpp>

template <typename T1, typename T2, typename T3>
inline
typename boost::math::tools::promote_args<T1,T2,T3>::type
normal_log(const T1& y1, const T2& y2, const T3& y3) {
  using std::pow;  using std::log;  
  return -0.5 * pow((y - mu) / sigma, 2)
      - log(sigma)
      - 1 / sqrt(2 * pi<double>());
}
\end{Verbatim}
\end{quote}
%

\subsubsection{Argument-Dependent Lookup for Function Resolution}

In order to allow built-in functions such as \code{log()} and
\code{pow()} to be instantiated with arguments of type
\code{agrad::var} and primitive C++ types, the primitive version of
the function is brought in with a \code{using} statement, as in
\code{using~std::pow;}.  This brings the version of \code{pow()} that
applies to \code{double} arguments into scope.  The definition of
\code{pow()} for autodiff variables \code{stan::agrad} is brought in
through argument-dependent lookup
\cite[Section~3.4]{cpp-standard:2003}, which brings the namespace of
any argument variables into scope for the purposes of resolving
function applications.


\subsubsection{Traits Metaprogram for Computing Return Types}

Boost's traits-based metaprogram \code{promote\_args}
\citep{Boost:2011} is designed to calculate return types for highly
templated functions like \code{normal\_log()}.

In general, for an automatically differentiated function, the return
type should be \code{double} if all the input arguments are primitive
integers or double-precision floating point values, and
\code{agrad::var} if any of the arguments is of type
\code{agrad::var}.

Given a sequence of types $\code{T1}, \ldots, \code{TN}$, the template
structure \code{promote\_args<T1,...,TN> } defines a typedef named
\code{type}, which is defined to be \code{double} if all the inputs
are \code{int} or \code{double}, and \code{agrad::var} if any of the
input types is \code{agrad::var}.  The keyword \code{typename} is
required to let the C++ parser know that the member variable is a
typedef rather than a regular value.  The Boost promotion mechanism is
used throughout the \code{stan::agrad} library to define both return
types and types of variables for intermediate results.  For instance,
\code{y~-~mu} would be assigned to an intermediate variable of type
\code{promote\_args<T1,T2>::type}.

The fully templated definition allows the template parameters
\code{T1}, \code{T2}, or \code{T3} to be instantiated as independent
variables\, \mbox{\rm (\code{agrad::var})} or constants\, \mbox{\rm
  (\code{double}, \code{int})}.  For example, it can be used in place
of the direct definitions in the previous programs as
%
\begin{quote}
\begin{Verbatim}
...
double y = 1.3;
stan::agrad::var mu = 0.5, sigma = 1.2;

stan::agrad::var lp = normal_log(y,mu,sigma);
...
\end{Verbatim}
\end{quote}




\subsection{Calculating the Derivatives of Functors with Functionals}

The \code{stan::agrad} library provides a fully abstracted approach to
automatic differentiation that uses a C++ functor to represent a
function to be differentiated and a functional for the gradient
operator.  

\subsubsection{Eigen C++ Library for Matrices and Linear Algebra} 

Here and throughout \code{stan::agrad}, the Eigen C++ library for
matrix operations and linear algebra is used \cite{Eigen:2013}.

The type \code{Matrix<T,R,C>} is the Eigen type for matrices
containing elements of type \code{T}, with row and column type
\code{C}.  The \code{stan::agrad} library uses three possible
instantiations, \code{Matrix<T,Dynamic,1>} for (column) vectors,
\code{Matrix<T,1,Dynamic>} for row vectors, and
\code{Matrix<T,Dynamic,Dynamic>} for matrices.  These three instances
are all specialized with their own operators in \code{Eigen}.  Like
the standard template library's \code{std::vector} class, these all
allocate memory for elements dynamically on the C++ heap.

\subsubsection{Definining Functors in C++}

A functor in C++ is a function that defines \code{operator()} and can
hence behaves syntactically like a function.  For example, the normal log
likelihood may be defined directly as a functor as follows.
%
\begin{quote}
\begin{Verbatim}
#include <stan/agrad/autodiff.hpp>

using Eigen::Matrix;  using Eigen::Dynamic;

struct normal_ll {
  const Matrix<double,Dynamic,1> y_;

  normal_ll(const std::vector<double>& y) : y_(y) { }

  template <typename T>
  T operator()(const Matrix<T,Dynamic,1>& theta) {
    T mu = theta[0];   T sigma = theta[1];
    T lp = 0;
    for (size_t n = 0; n < y_.size(); ++n)
      lp += normal_log(y_[n],mu,sigma);
    return lp;
  }
};
\end{Verbatim}
\end{quote}
%
The variable \code{y\_} is used to store the data vector in the
structure.  The \code{operator()} defines a function whose argument
argument type is the Eigen type for vectors with elements of type
\code{T}; the result is also defined to be of type \code{T}. 


\subsubsection{Functionals in C++}

A functional in C++ is a function that applies to a functor.  For
functional autodiff in \code{stan::agrad}, the functor must define an
\code{operator()} that can be applied to an Eigen vector containing
elements of type \code{agrad::var}.  In the running example, the
elements of the vector argument are declared to have the type of the
template parameter \code{T}, which can be instantiated to
\code{agrad::var} for differentiation and to \code{double} for
testing.

The following code calculates the gradient of the functor at a
specified input point.
%
\begin{quote}
\begin{Verbatim}
Matrix<double,Dynamic,1> y(3);
y << 1.3, 2.7, -1.9;
normal_ll f(y);

Matrix<double,Dynamic,1> theta(2);
theta << 1.3, 2.9;

Matrix<double,Dynamic,1> grad_fx;
double fx;
stan::agrad::gradient(f, x, fx, grad_fx);
\end{Verbatim}
\end{quote}
%
The log likelihood function is instantiated as \code{f} with a vector
of data.  The argument \code{theta} is instantiated with the parameter
values.  Then a scalar \code{fx} is defined to hold the function value
and an Eigen vector \code{grad\_fx} is defined to hold the gradient.
The function \code{gradient} is then called to calculate the value and
the gradient from the function \code{f} and input \code{x}.


\subsection{Calculating Jacobians}

Mathematically, Jacobians are simply repeated gradient calculations,
one for each output of a multivariate function; see
\refsection{derivative-definitions} for an exact definition.
Computationally, they work the same way, requiring a single forward
pass to construct all of the outputs, then one reverse pass for each
output dimension.

\subsubsection{Direct Jacobian Calculation}

Suppose that \code{f} is a functor that accepts an dimensional Eigen
$N$-vector of autodiff variables as input (typically through
templating) and produces an Eigen $M$-vector as output.
The following code calculates the Jacobian of \code{f} evaluated 
at the input \code{x}.

\begin{quote}
\begin{Verbatim}
Matrix<double,Dynamic,1> x = ...;   // inputs

Matrix<var,Dynamic,1> x_var(x.size());
for (int i = 0; i < x.size(); ++i) x_var(i) = x(i);

Matrix<var,Dynamic,1> f_x_var = f(x_var);

Matrix<double,Dynamic,1> f_x(f_x_var.size());
for (int i = 0; i < f_x.size(); ++i) f_x(i) = f_x_var(i).val();

Matrix<double,Dynamic,Dynamic> J;
for (int i = 0; i < f_x_var.size(); ++i) {
  if (i > 0) stan::agrad::set_zero_all_adjoints();
  fx_var(i).grad();
  for (int j = 0; j < x_var.size(); ++j)
    J(i,j) = x_var(j).adj();
}
\end{Verbatim}
\end{quote}
%
First, the arguments are used to create autodiff variables.  Next, the
function is applied and the result is converted back to a vector of
doubles.  Then for each output dimension, automatic differentiation
calculates the gradient of that dimension and uses it to populate a
row of the Jacobian.  After the first gradient is calculated, all
subsequent gradient calculations begin by setting the adjoints to
zero; this is not required for the frist gradient because the adjoints
are initialized to zero.


\subsubsection{Functional Jacobian Calculation}

Alternatively, the Jacobian functional can be applied directly to the
function \code{f}, as follows.
%
\begin{quote}
\begin{Verbatim}
...
Matrix<double,Dynamic,Dynamic> J;
matrix<double,Dynamic,1> f_x;
stan::agrad::jacobian(f, x, f_x, J);
\end{Verbatim}
\end{quote}




\section{Automatic Differentiation Variable Base Classes}\label{autodiff-base-classes.section}

As demonstrated in the previous section, the client-facing data type
of \code{stan::agrad} is the type \code{stan::agrad::var}, which is
used in place of primitive \code{double} values in functions that are
to be automatically differentiated.  

\subsection{Pointers to Implementations}

The \code{var} class is implemented following the pointer to
implementation (Pimpl) pattern \citep{sutter:98,sutter:01}.  The
implementation class here is of type \code{vari}, and is covered in
the next subsection.  Like the factory pattern, the Pimpl pattern
encapsulates the details of the implementation class(es), which the
API need not expose to clients.  For example, none of the example
programs in the last section involved the \code{vari} type explicitly.
Pimpl classes also encapsulate the messy and error-prone pointer
manipulatiosn required for allocating memory.  

\subsection{Resource Allocation is Initialization}

Like many Pimpl classes, \code{var} manages memory using a mechanism
in which resource allocation is initialization (RAII) pattern
\citep{stroustrup:94}.  With RAII, instances are managed on the stack
and passed to functions just like primitive types.  Memory management
is handled behind the scenes of the client-facing application
programmer interface (API).

Other classes implemented using Pimpl and RAII include
\code{std::vector} and \code{Eigen::Matrix}, both of which allocate
memory directly on the C++ heap and deallocate it when their variables
go out of scope and their destructors are called.  As explained below,
memory is managed slightly differently for \code{agrad::var}
instances.

\subsection{The \code{var} Class}

The core of the \code{stan::agrad::var} class is just implemented as a
pointer to implementations.
%
\begin{quote}
\begin{Verbatim}
class var {
private:
  vari* vi_;
public:
  var() : vi_(static_cast<vari*>(0U)) { }
  var(double v) : vi_(new vari(v)) { }

  double val() const { return vi_->val_; }
  double adj() const { return vi_->adj_; }  
};
\end{Verbatim}
\end{quote}
%
The defualt constructor \code{var()} produces a null pointer.  When
instantiated with a \code{double} value, a new \code{vari} instance is
constructed for the value.  The memory management for \code{vari} is
handled through a specialization of \code{operator}~\code{new} as
described below.

The \code{var} class additionally defines a copy constructor,
constructors for the other primitive types, and the full set of
assignment and compound assignment operators.  The destructor is
implicit because there is nothing to do to clean up instances; all of
the work is done with the memory management for \code{vari}. The class
is defined in the \code{stan::agrad} namespace, but the namespace
declarations are not shown to save
space.

\subsection{The \code{chainable} Base Class}

The design of \code{stan::agrad}'s variable implementation classes is
object-oriented, in that subclasses extend \code{chainable} and implement
the virtual method \code{chain()} in a function-specific way to
propagate derivatives with the chain rule.  The class \code{vari}
itself extends the base class \code{chainable}, and holds the variable
value and adjoint.  Extensisons of \code{vari} will hold pointers to
operands for propagating derivatives and sometimes store extra
information needed for the partial derivative calculations.

\begin{quote}
\begin{Verbatim}
struct chainable {
  chainable() { }
  virtual ~chainable() { }

  virtual void chain() { }
  virtual void init_dependent() { }
  virtual void set_zero_adjoint() { }

  static inline void* operator new(size_t nbytes) {
    return ChainableStack::memalloc_.alloc(nbytes);
  }
};
\end{Verbatim}
\end{quote}
%
The core of the \code{agrad::chainable} base class.  The virtual
method \code{chain()} implements derivative propagation for extensions
of \code{chainable}.  There is a static specialization of
\code{operator}~\code{new}, which handles custom arena-based memory
management.  The initialization and set-zero are virtual methods used
to initialize the adjoints during the derivative propagation (reverse)
pass.

\subsection{The \code{vari} Class}

The core of the \code{agrad::vari} class, which extends the base class
\code{agrad::chainable} and holds the actual value and adjoint for an
autodiff variable instantiation.  namespace.  
%
\begin{quote}
\begin{Verbatim}
class vari : public chainable {
public:
  const double val_;
  double adj_;

  vari(double v) : val_(v), adj_(0) { 
    ChainableStack::var_stack_.push_back(this);
  }

  virtual ~vari() { }

  virtual void init_dependent() { adj_ = 1; }
  virtual void set_zero_adjoint() { adj_ = 0; }
};
\end{Verbatim}
\end{quote}
%
The adjoint manipulation
methods are defined to set the adjoint to 1 when initializing the
dependent variable's adjoint and 0 for the set-to-zero
operation. There is a further constructor that manages memory slightly
differently that will be discussed
later.

\section{Calculating Gradients}

There is a static method in the \code{agrad::chainable} class which
performs the derivative propagation.
%
\begin{quote}
\begin{Verbatim}
static void grad(chainable* vi) {
  typedef std::vector<chainable*>::reverse_iterator it_t;
  vi->init_dependent(); 
  it_t begin = ChainableStack::var_stack_.rbegin();
  it_t end = ChainableStack::var_stack_.rend();
  for (it_t it = begin; it < end; ++it)
    (*it)->chain();
}
\end{Verbatim}
\end{quote}
%
The function initializes the dependent variable to 1 using the
\code{init\_dependent} method on \code{chainable}.  Then a reverse
iterator is used to iterate over the variable stack from the top
(\code{rbegin}) down to the bottom (\code{rend}), executing each
\code{chainable} instance's \code{chain()} method.  Because the stack
of \code{chainable} instances is sorted in topological order, each
node's parents (superexpressions in which it directly appears) will
have all been visited by the time the node's chain-rule propagation
method is called.

\subsection{Jacobians and Zeroing Adjoints}

To calculate Jacobians efficiently, the same expression graph is used
to calculate each row of the Jacobian.  Between each gradient call,
the static method \code{agrad::set\_zero\_all\_adjoints()} is called for
each row after the first.  This function walks over the variable stack
\code{ChainableStack::var\_stack\_}, resetting each value to zero using
the virtual method \code{set\_zero\_adjoint}.
%
\begin{quote}
\begin{Verbatim}
static void set_zero_all_adjoints() {
  for (size_t i = 0; i < ChainableStack::var_stack_.size(); ++i)
    ChainableStack::var_stack_[i]->set_zero_adjoint();
}
\end{Verbatim}
\end{quote}
%
As in all of the client-facing API methods, pointer and global
variable manipulation in encapsulated.

% THIS FUNCTION SHOULD BE REWRITTEN WITH ITERATORS FOR CONSISTENCY


\section{Memory Management}

Instances of \code{vari} and its extensions are allocated using the
specialized static \code{operator}~\code{new} definition.  The specialization
references the \code{alloc(size\_t)} method of the variable
\code{ChainableStack::memalloc\_}, which provides a custom arena-based
memory management for variable implementations.


The core of the \code{agrad::ChainableStack} template class
definition, which holds the memory arena for reverse-mode autodiff, is
as follows.
%
\begin{quote}
\begin{Verbatim}
template<typename T, typename AllocT>
struct AutodiffStackStorage {
  static std::vector<T*> var_stack_;
  static std::vector<AllocT*> var_alloc_stack_;
  static memory::stack_alloc memalloc_;
};

struct chainable_alloc {
  chainable_alloc() {
    ChainableStack::var_alloc_stack_.push_back(this);
  }
  virtual ~chainable_alloc() { };
};

typedef AutodiffStackStorage<chainable,chainable_alloc>
  ChainableStack;
\end{Verbatim}
\end{quote}
%
The variables are all declared static, so they serve as global
variables in the program.  The variable \code{var\_stack\_}, holds all
of the usual autodiff variables and is traversed to propagate
derivatives.  The variable \code{memalloc\_} holds the byte-level
memory allocator, which is described in the next section.  

The typedef \code{ChainableStack} specifies the template parameters
for the template class \code{AutodiffStackStorage}.   Rather than
creating instances,  the global variables are accessed through the
typedef instantiation (e.g., \code{ChainableStack::var\_stack\_}).

The \code{var\_alloc\_stack} variable holds objects allocated for
autodiff that need to have their \code{delete()} methods called.  Such
variables are required to specialize \code{chainable\_alloc}, which
pushes them onto the \code{var\_alloc\_stack\_} during construction.
The destructor is virtual because different instances will sit on the
\code{var\_alloc\_stack\_}.





\subsection{Byte-Level Memory Manager:  \code{memory::stack\_alloc}}

The byte-level memory manager uses an arena-based strategy where
blocks of memory are allocated and used to hold the \code{vari}
instances created during the forward expression-graph building pass of
automatic differentiation.  After the partials are propagated and final
gradients calculated in the reverse pass, the memory for the
expression graph is reclaimed all at once.

The actual storage used is a standard vector of \code{char*} blocks.  
New variables are filled into the blocks until they will no longer
fit, at which point a new block is allocated.  The basic data
structures used are defined in the \code{memory::stack\_alloc} class,
the member variables of which are defined as follows.
%
\begin{quote}
\begin{Verbatim}
class stack_alloc {
private: 
  std::vector<char*> blocks_;
  std::vector<size_t> sizes_;
  size_t cur_block_;
  char* cur_block_end_;
  char* next_loc_;
  ...
\end{Verbatim}
\end{quote}
%
The variable \code{blocks\_} holds the actual memory.  
The parallel vector \code{sizes\_} stores the sizeof each block.
Then the variables indicate the index of the current block in
\code{blocks\_}, the end of the current block's available memory, and
the next location within the current block to place a new variable
implementation.  

Given a request for a given block of memory, if it fits in the
current block, that is where it goes.  If it doesn't fit, a new block
is allocated that is twice the size of the current block and all the
indexes to the current position are updated.  

Because memory allocation is such a low-level operation in the
algorithm and because CPU branch misprediction is so expensive, the
ability of the GNU compiler family to accept guidance in terms of how
to sequence generated assembly code is used through the following
macros.
%
\begin{quote}
\begin{Verbatim}
#ifdef __GNUC__
#define likely(x)      __builtin_expect(!!(x), 1)
#define unlikely(x)    __builtin_expect(!!(x), 0)
#else
#define likely(x)     (x)
#define unlikely(x)   (x)
#endif
\end{Verbatim}
\end{quote}
%
Then a condition is wrapped in \code{likely(...)} or \code{unlikely(...)} to
give the compiler a hint as to which branch to predict by default.
This improved throughput by a factor of nearly 10\% compared to the
unadorned code.    The allocation method in \code{stack\_alloc} is
defined as
%
\begin{quote}
\begin{Verbatim}
inline void* alloc(size_t len) {
  char* result = next_loc_;
  next_loc_ += len;
  if (unlikely(next_loc_ >= cur_block_end_))
    result = move_to_next_block(len);
  return (void*)result;
}
\end{Verbatim}
\end{quote}
%
Thus running out of memory in a block is marked as unlikely.  

The \code{move\_to\_next\_block} method, not shown here, allocates a
new block of memory and updates the underlying memory stacks.  No
movement of existing data is required and thus no additional memory
overhead is required to store the original and copy.  All of the
memory allocation is also done to ensure 8-byte alignment for maximum
access speed of pointers in 64-bit architectures.

\subsection{Memory Lifecycle}

An instance of \code{vari} is constructed for each subexpression
evaluated in the program that computes the function to be
automatically differentiated.  For each \code{vari} constructed,
memory is allocated in the arena and a pointer to the memory allocated
is pushed onto the variable stack.  Both the bytes and the stack of
pointers must persist until the call to \code{grad} is used to compute
derivatives, at which point it may be recovered.

The \code{AutodiffStackStorage} class holds the variable stacks and
the an instance of \code{memory::stack\_alloc} for the bytes used to
create \code{chainable} instances.  The \code{AutodiffStackStorage}
class also implements a no-argument \code{recover\_memory()} method
which frees the stack of pointers making up the variable stack, and
resets the byte-level \code{stack\_alloc} instance.  The underlying
blocks of memory allocated within the \code{stack\_alloc} instance are
intentionally \emph{not} freed by the \code{recover\_memory()} method.
Rather, the underlying memory pool is saved and reused when the next
call to automatic differentiation is made.  A method is available to
clients in the \code{stack\_alloc} class to completely free the
underlying memory pools.

Within the functional implementations of automatic differentiation,
the memory is all handled with exception-safe memory recovery that
ensures whenver a derivative functional is called memory cannot leak
even if the functor throws an exception.  The functionals do not free
the underlying memory pool, but clients can do that manually after
functionals are called if desired.


\section{Functionals}

The functional \code{gradient()} encapsulates the gradient
calculations shown in the direct implementation as follows (the
namespace qualificaations of functions and classes are omitted for
readability).
%
\begin{quote}
\begin{Verbatim}
template <typename F> 
void gradient(const F& f,
              const Matrix<double,Dynamic,1>& x,
              double& fx,
              Matrix<double,Dynamic,1>& grad_fx) {
  try {
    Matrix<var,Dynamic,1> x_var(x.size());
    for (int i = 0; i < x.size(); ++i)
      x_var(i) = x(i);
    var fx_var = f(x_var);
    fx = fx_var.val();
    grad(fx_var.vi_);
    grad_fx.resize(x.size());
    for (int i = 0; i < x.size(); ++i)
      grad_fx(i) = x_var(i).adj();
  } catch (const std::exception& /*e*/) {
    recover_memory();
    throw;
  }
  recover_memory();
}
\end{Verbatim}
\end{quote}
%
Client code, as our earlier example shows, does not need to specify
the template parameter \code{F} explicitly, because it can be
automatically inferred from the static type of the argument functor
\code{f}.  Because the functor argument reference is declared
\code{const} here, its \code{operator()} implementation must also be
marked \code{const}.  The final two arguments, \code{fx} and
\code{grad\_fx} will be set to the value of the function and the value
of the gradient respectively.  

All of the code is wrapped in a \code{try} block with a corresponding
\code{catch} that recovers memory and rethrows whatever standard
exception was caught.  If no exception is thrown in the block of the
\code{try}, then the function terminates with a call to recover
memory (which ony resets memory---it does not free any of the
underlying blocks that have been allocated, saving them for reuse).

The main body of the algorithm first copies the input vector of
\code{double} values, \code{x}, into a vector of \code{var} values,
\code{x\_var}.  Then the functor \code{f} is applied to this newly
created vector \code{x\_var}, resultinng in a type \code{var} result.
This usage requires \code{f} to implement \code{operator()} that
returns a \code{var} and takes as input a single vector of \code{var}.
Any other information required to compute the function, such as data
vectors (\code{double} or \code{int} values) or external callbacks
such as a logging or error reporting mechanism, must be accessible to
the \code{operator()} definition the functor \code{f}.

The evaluation of \code{f} may throw an exception, which will result
in whatever memory has been allocated up to the exception to be
recovered.  After the function returns a \code{var}, its value is then
extracted and assigned to the argument reference \code{fx}.

Next, the return vector for the gradient, specified as argument
reference \code{grad\_fx}, is resized if necessary.  

Next, the static \code{grad} function is called directly on the
variable implementation of the result.  This resulting derivatives are
then extraced from the dependent variables in the argument vector and
assigned to the argument reference vector \code{grad\_fx}, resizing it
if necessary.



\section{Constants}

There are two equivalent ways to create constant autodiff variables
from constant \code{double} values.  The first approach uses the unary
constructor for \code{var} instances, which creates a new \code{var}
instance \code{sigma} on the C++ function-call stack.
%
\begin{quote}
\begin{Verbatim}
var sigma(0.0);
\end{Verbatim}
\end{quote}
%
As shown in \refsection{autodiff-base-classes}, the underlying
\code{vari} implemenetation to which the \code{var} points will
construct a new instance, the memory for which will be allocated in
the memory arena.

The second approach to constructing a constant autodiff variable uses
assignment.  Because the unary constructor of \code{var} is implicit
(i.e., not declared \code{explicit}), assignment of a \code{double} to
a \code{var} will construct a new \code{var} instance.  
%
\begin{quote}
\begin{Verbatim}
var tau = 0.0;
\end{Verbatim}
\end{quote}
%
The behavior of \code{sigma} and \code{tau} are the same because they
have exacty the same effect on memory and the autodiff stack.

It is also possible to construct instances using integers or other
base types such as \code{unsigned long} or \code{float}.  The
additional constructors are not shown.  Because the copy constructor
is declared explicit, there is no ambiguity in the constructors.

A \code{vari} inherits the no-op \code{chain()} method implementation
from its superclass \code{chainable}.  Therefore, a constant will not
propagate any derivative information.  Nevertheless, it is more
efficient not to construct \code{var} instances out of double values
and just use the \code{double} value directly, because
\code{stan::agrad} specializes all of its functions and operators
allow free variation between \code{var} and \code{double} argumens.


\section{Functions and Operators}

This section describes how functions are implemented in
\code{stan::agrad}, beginning with an illustration of the basic
mechanisms using a unary function and then moving on to binary functions.

\subsection{Unary Operand Storage Class}

Most of the unary functions in \code{stan::agrad} are implemented as
extensions of a simple helper class, \code{op\_v\_vari}, which stores
the value of a unary function and its operand.  
%
\begin{quote}
\begin{Verbatim}
struct op_v_vari : public vari {
  vari* avi_;

  op_v_vari(double f, vari* avi) : vari(f), avi_(avi) { }
};
\end{Verbatim}
\end{quote}
%
Constructing a \code{op\_v\_vari} instance passes the function value
\code{f} to the superclass constructor, which stores it in member
variable \code{val\_}.  The second argument to the constructor is a
\code{vari} pointer, which is stored here in the member variable
\code{avi\_}.  The \code{vari} will be set to The \code{op\_v\_vari}
inherits the no-op \code{chain()} method from \code{chainable}.

\subsection{Unary Function Implementation}

As a running example, consider the natural logarithm function, the
derivative of which is
%
\[
\log'(x) = \frac{1}{x}.
\]
%
The implementation of the \code{log} function for \code{var}
constructs a specialized \code{vari} instance as follows.
%
\begin{quote}
\begin{Verbatim}
inline var log(const var& a) {
  return var(new log_vari(a.vi_));
}
\end{Verbatim}
\end{quote}
%
The argument \code{a.vi\_} supplied to the constructor is the variable
implementation of the argument variable \code{a}, which is the operand
for the \code{log} function.

The class \code{log\_vari} is defined as follows.
%
\begin{quote}
\begin{Verbatim}
struct log_vari : public op_v_vari {
  log_vari(vari* avi) :
    op_v_vari(std::log(avi->val_),avi) { }

  void chain() {
    avi_->adj_ += adj_ / avi_->val_;
  }
};
\end{Verbatim}
\end{quote}
%
The constructor takes a pointer, \code{avi}, to the operand's
implementation.  The value of the operand is given by
\code{avi->val\_}.  The log of the operand's value is computed using
\code{std::log()} and passed to the \code{op\_v\_vari} constructor,
which stores it, along with the operand implementation itself,
\code{avi}.  

The virtual \code{chain()} method of the superclass \code{chainable}
is overriden to divide the variable's adjoint, \code{adj\_}, by the
operand's value, \code{avi\_->val\_}, and use the result to increment
the operand's adjoint, \code{avi\_->adj\_}.  This corresponds to
\code{+=} operations detailed in \reffigure{autodiff-stack}, in
particular the one for the log given by $a_3 += a_8 \times (1 / v_3)$,
in which $a_3$ and $v_3$ are the adjoint and value of the operand and
$a_8$ the adjoint of the result.

The \code{chain()} method is not called until the reverse pass of
automatic differentiation is executed, so the derivative $1/x$, here
given by the divison \code{/~avi\_->val\_} is both calculated lazily
and also optimized versus the naive implementation which multiplied
\code{adj\_} by the partial \code{1~/~avi\_->val\_}.  

Most other functions are implemented in the same way as \code{log},
with their own specialized \code{chain()} implementation and storage.
The savings for operations like addition and subtraction are even
larger because there is no need to do any multiplication at all.

\subsection{Binary Functions}

As an example, consider the \code{pow()} function, defined by
\[
\mbox{pow}(x,y) = x^y,
\]
with derivatives
\[
\frac{d}{dx} x^y = y \, x^{y-1}
\]
and
\[
\frac{d}{dy} x^y = x^y \log x.
\]


\subsubsection{Binary Operands Storage Class}

Not surprisingly binary functions of two autodiff variables are
implemented using a specialized \code{vari} implementation.
%
\begin{quote}
\begin{Verbatim}
struct op_vv_vari : public vari {
  vari* avi_;   vari* bvi_;

  op_vv_vari(double f, vari* avi, vari* bvi)
  : vari(f), avi_(avi), bvi_(bvi) { }
};
\end{Verbatim}
\end{quote}
%
Like its unary counterpart, this class stores the value (\code{f}) and
pointers to the operands (\code{avi\_}, \code{bvi\_}).  The number of
operands need not be stored explicitly because it is implicit in the
identity of the class. 

For mixed operations of \code{var} and \code{double} variables, the
following base \code{vari} is provided.
%
\begin{quote}
\begin{Verbatim}
struct op_vd_vari : public vari {
  vari* avi_;
  double bd_;

  op_vd_vari(double f, vari* avi, double b)
  : vari(f), avi_(avi), bd_(b) {  }
};
\end{Verbatim}
\end{quote}
%
Although not strictly necessary given the above, the following
symmetric class is provided for naming convenience.
%
\begin{quote}
\begin{Verbatim}
struct op_dv_vari : public vari {
  double ad_;
  vari* bvi_;

  op_dv_vari(double f, double a, vari* bvi)
  : vari(f), ad_(a), bvi_(bvi) { }
};
\end{Verbatim}
\end{quote}
%

\subsubsection{Binary Function Implementation}

There are three function implementations for \code{pow()} based on the
type of the arguments, and each has its own specialized
implementation, the first of of which follows the same pattern as
\code{log}, only with two arguments.
%
\begin{quote}
\begin{Verbatim}
inline var pow(const var& base, const var& exponent) {
  return var(new pow_vv_vari(base.vi_,exponent.vi_));
}
\end{Verbatim}
\end{quote}
%
The second specialization is similar, only the \code{double} argument
is passed by value to the specialized \code{vari} constructor.
%
\begin{quote}
\begin{Verbatim}
inline var pow(double base, const var& exponent) {
  return var(new pow_dv_vari(base,exponent.vi_));
}
\end{Verbatim}
\end{quote}
%
The last implementation with a \code{double} exponent first checks if
there is a built-in special case that can be used, and if not,
constructs a specialized \code{vari} for power.
%
\begin{quote}
\begin{Verbatim}
inline var pow(const var& base, double exponent) {
  if (exponent == 0.5) return sqrt(base);
  if (exponent == 1.0) return base;
  if (exponent == 2.0) return square(base);
  return var(new pow_vd_vari(base.vi_,exponent));
}
\end{Verbatim}
\end{quote}
%
By using \code{sqrt}, \code{square}, or a no-op, less memory is
allocated and fewer arithmetic operations are needed during the
evaluation of \code{chain()} for the remaining expressions, thus
saving both time and space.

\subsubsection{Variable Implementations}

The \code{pow\_vv\_vari} class for \code{pow(var,var)} is defined as follows.
%
\begin{quote}
\begin{Verbatim}
struct pow_vv_vari : public op_vv_vari {
  pow_vv_vari(vari* avi, vari* bvi)
  : op_vv_vari(std::pow(avi->val_,bvi->val_),avi,bvi) { }

  void chain() {
    if (avi_->val_ == 0.0) return; 
    avi_->adj_ += adj_ * bvi_->val_ * val_ / avi_->val_;
    bvi_->adj_ += adj_ * std::log(avi_->val_) * val_;
  }
};
\end{Verbatim}
\end{quote}
%
The constructor calls the \code{op\_vv\_vari} constructor to store the
value and the two operand pointers.  The \code{chain()} implementation
first tests if the base is 0, and returns because the derivatives are
both zero, and thus there is nothing to do for the chain rule.  This
test is not for efficiency, but rather because it avoids evaluating
the logarithm of zero, which evaluates to special not-a-number value
in IEEE floating-point arithmetic.%
%
\footnote{An even more efficient alternative
would be to evaluate this condition ahead of time and just return a
constant zero \code{var} at the top level.}
%
In the normal case of execution when the base is not zero, the
\code{chain()} method increments the operands' adjoints based on the
derivatives given above.  The expression for the derivative for the
power operand (\code{bvi\_}) conveniently involves the value of the
function itself, just as \code{exp} did.

The implementations for mixed inputs simply compute a single
derivative propagation.
%
\begin{quote}
\begin{Verbatim}
struct pow_vd_vari : public op_vd_vari {
  pow_vd_vari(vari* avi, double b) :
    op_vd_vari(std::pow(avi->val_,b),avi,b) {
  }

  void chain() {
    avi_->adj_ += adj_ * bd_ * val_ / avi_->val_;
  }
};
\end{Verbatim}
\end{quote}
%
The base argument (\code{avi}) is a variable implementation pointer
that is stored and accessed in \code{chain()} as \code{avi\_}.  The
exponent argument (\code{b}) is stored and accessed as \code{bd\_}.  


\subsection{Arithmetic Operators}

Basic arithmetic operators are implemented in exactly the same way as
functions.  For example, addition of two variables is implemented
using a support class \code{add\_vv\_vari}.
%
\begin{quote}
\begin{Verbatim}
inline var operator+(const var& a, const var& b) {    
  return var(new add_vv_vari(a.vi_,b.vi_));
}
\end{Verbatim}
\end{quote}
%
The implementation of \code{add\_vv\_vari} follows that of \code{exp},
with the same naming conventions.  The derivatives are
\[
\frac{d}{dx} \left( x + y \right) = 1
\]
and
\[
\frac{d}{dy} \left( x + y \right) = 1.
\]
As a result, the \code{chain()} method can be reduced to just adding
the expression's adjoints to that of its operands.
%
\begin{quote}
\begin{Verbatim}
void chain() {
  avi_->adj_ += adj_;
  bvi_->adj_ += adj_;
}
\end{Verbatim}
\end{quote}
%
The mixed input operators are specialized in the same way as the mixed
input functions.  The other built-in operators for unary subtraction,
multiplication, and division are implemented similarly.

\subsection{Boolean Operators}

The boolean operators are implemented directly without creating new
variable implementation instances.  For example, equality of two
variables is implemented as follows.
%
\begin{quote}
\begin{Verbatim}
inline bool operator==(const var& a, const var& b) {
  return a.val() == b.val();
}
\end{Verbatim}
\end{quote}
%
Mixed input types are handled similarly.
%
\begin{quote}
\begin{Verbatim}
inline bool operator==(const var& a, double b) {
  return a.val() == b;
}
\end{Verbatim}
\end{quote}
%
The other boolean operators are handled similarly.



\subsection{Memory Usage}

The amount of memory required for automatic differentiation is the sum
of the memory required for each expression in the expression graph.
These expressions are represented with data structures like
\code{log\_vari}.  The storage cost in the arena is as follows for a
function with $K$ operands.
%
\begin{center}
\begin{tabular}{r|cc}
{\it Description} & {\it Type} & {\it Size (bytes)}
\\ \hline
value & \code{double} & 8
\\
operand pointers & \code{size\_t} & $K \times 8$
\\
vtable pointer  & \code{size\_t} & 8
\\
\code{var\_stack\_} ptr & \code{size\_t} & 8
\\ \hline \hline
{\sc total} & & 24 + $K \times 8$
\end{tabular}
\end{center}
%
Each expression stores its value using double-precision floating
point, requiring 8 bytes.  Pointers are stored to the operand(s),
requiring 8 bytes per pointer.  The number of operands is not stored,
but is rather implicit in the identity of the class used for the
\code{vari} and how it computes \code{chain()}.

Although there are 8 bytes of required for the \code{var} instances
for the pointer to the implementation member variable (\code{vari*}),
this is no more memory than would be used for the \code{vari*} itself.
Wrapping the \code{vari*} in a pointer to implementation class does
not increase memory usage.  Furthermore, many \code{var} instances
will be allocated on the stack rather than on the heap; the only heap
allocation required is for use in containers, such as standard vectors
or Eigen matrices.  As with uses on the stack, there is no additional
memory overhead for the \code{var} instance over directly storing a
pointer or reference.

In addition to the memory required for the \code{vari} instances, each
expression gets a pointer on the variable stack, \code{var\_stack\_}.
The price for object orientation in the virtual \code{chain()} method
is 8 bytes for the vtable pointer, which C++ includes in any object
with a virtual method so that it can dynamically resolve the
implementation of the virtual method.  The cost for storing a
heterogeneous collection of expressions on the variable stack is
either a virtual function lookup using the vtable or pointer chasing
to implementations with function pointers.



\section{Assignment and Compound Assignment}

\subsection{Assignment Operator}

The assignment operator, \code{operator=}, is defined implicitly for
\code{var} types.  Default assignment of \code{var} instances to
\code{var} instances is handled through the C++ defaults, which
perform a shallow copy of all member variables, in this case copying
the \code{vari*} pointer.

For assigning \code{double} values to \code{var} instances, the
implicit constructor \code{var(double)} is used to construct a new
variable instance and its \code{vari*} pointer is copied into the
left-hand side \code{var}.  

Assigning \code{var} typed expressions to \code{double} variables is
forbidden.  

\subsection{Compound Assignment Operators}

The compound assignment operators such as \code{+=} are used as
follows.
%
\begin{quote}
\begin{Verbatim}
var a = 3;
var b = 4;
a += b;
\end{Verbatim}
\end{quote}
%
and are intended to have the same effect as
%
\begin{quote}
\begin{Verbatim}
a = a + b;
\end{Verbatim}
\end{quote}
%
The compound operators are declared as member functions in \code{var}.
%
\begin{quote}
\begin{Verbatim}
struct var {
...
  inline var& operator*=(const var& b);
  inline var& operator*=(double b);

};
\end{Verbatim}
\end{quote}
%
The left-hand side is implicitly the variable in which the operators
are declared.

The implementations are defined outside the class declaration.  For variable
arguments, the definition is as follows; the implementation for
\code{double} arguments is similar.
%
\begin{quote}
\begin{Verbatim}
inline var& var::operator*=(const var& b) {
  vi_ = new multiply_vv_vari(vi_,b.vi_);
  return *this;
}
\end{Verbatim}
\end{quote}
%
A new \code{multiply\_vv\_vari} instance is created and the pointer to
it assigned to the member variable \code{vi\_} of the left-hand side
variable in the compound assignment.  The expression \code{*this} is
the value of the left-hand side variable in the assignment, which will
be returned by reference according to the declared return type
\code{var\&}.

The effect of \code{a~+=~b;} is to modify \code{a}, assigning it to
\code{a~+~b}.  This is a destructive operation on \code{var} instances
such as \code{a}, but it is not a destructive operation on variable
implementations.  A sequence such as 
%
\begin{quote}
\begin{Verbatim}
var a = 0;
a + = b * b;
a + = c * c;
\end{Verbatim}
\end{quote}
%
winds up creating the same expression graph as
%
\begin{quote}
\begin{Verbatim}
var a = b * b + c * c;
\end{Verbatim}
\end{quote}
%
with the same value pointed to by \code{a.vi\_} in the end.


\section{Variadic Functions}\label{variadic-functions.section}

Some functions can take an arbitrary number of arguments.  One example
that is useful in statistical models to prevent overflow and underflow
is the log sum of exponentials operation, defined for an $N$-vector
$x$ by
%
\[
\mbox{log\_sum\_exp}(x) 
= \log \sum_{n=1}^N \exp(x_n).
\]
%
The function is symmetric in its arguments and the derivative with
respect to $x_n$ is
\[
\frac{d}{d x_n} \mbox{log\_sum\_exp}(x)
= \frac{\exp(x_n)}{\sum_{n=1}^N \exp(x_n)}.
\]

For variadic functions, \code{op\_vector\_vari} provides a base class
with member variables for the array of operands and the array's size.
%
\begin{quote}
\begin{Verbatim}
struct op_vector_vari : public vari {
  const size_t size_;
  vari** vis_;

  op_vector_vari(double f, 
             const std::vector<stan::agrad::var>& vs) 
  : vari(f), size_(vs.size()), 
    vis_(static_cast<vari**>(operator new(sizeof(vari*) 
                                          * vs.size()))) {
    for (size_t i = 0; i < vs.size(); ++i)
      vis_[i] = vs[i].vi_;
  }
};
\end{Verbatim}
\end{quote}
%
Because it is used in an extension of \code{vari}, the specialized
memory arena \code{operator}~\code{new} is used to allocate the
memory.  Rather than use a \code{std::vector}, which allocates memory
on the standard C++ heap, using the memory arena avoids fragmentation
and also avoids having to call the destructor for \code{vari} extensions.

The log sum of exponential operator uses an extension that calculates
both the function value and the chain rule using external calls.
%
\begin{quote}
\begin{Verbatim}
struct log_sum_exp_vector_vari : public op_vector_vari {
  log_sum_exp_vector_vari(const std::vector<var>& x)
  : op_vector_vari(log_sum_exp_as_double(x), x) { }

  void chain() {
    for (size_t i = 0; i < size_; ++i)
      vis_[i]->adj_ += adj_ * std::exp(vis_[i]->val_ - val_);
  }
};
\end{Verbatim}
\end{quote}
%
The \code{chain()} method here loops over the operand pointers stored
in the array \code{vis\_}, and for each one, increments its
adjoint with the adjoint of the result times the derivative with
resepct to the operand, as given by
%
\[
\exp(\mbox{log\_sum\_exp}(x) - x_n)
\ = \
\frac{\exp(\mbox{log\_sum\_exp}(x))}{\exp(x_n)}
\ = \
\frac{\sum_{n=1}^N \exp(x_n)}
     {\exp(x_n)}.
\]
%
The function to compute the value, \code{log\_sum\_exp\_as\_double} is
not shown here;  it uses the usual algebraic trick to preserve the
most significant part of the result and avoid overflow on the
remaining calculations, by
%
\[
\log \sum_{n=1}^N \exp(x_n)
= \max(x) + \log \sum_{n=1}^N \exp(x_n - \max(x)).
\]
%
The data structure only requires storage for the operands and an
additional size pointer.  There is also only a single virtual function
call to \code{chain} that propagates derivatives for each argument.


\section{Matrix Data Structures and Arithmetic}

The \code{stan::agrad} library uses the Eigen C++ matrix library for
storing basic matrix data structures, performing matrix arithmetic,
and carrying out linear algebra operations such as calculating
determinants.

\subsection{Matrix Data Structures}

The design is intrinsically univariate, with matrix and vector types
being containers of univariate autodiff variables, i.e., instances of
\code{var}.  This is not required by the \code{chainable} base class,
but only \code{vari} instances are used in the library.

The \code{stan::agrad} library uses the following three Eigen types.
%
\begin{center}
\begin{tabular}{l|l}
{\it Type} & {\it Description}
\\ \hline
\code{Matrix<T,Dynamic,Dynamic>} & matrix
\\
\code{Matrix<T,Dynamic,1>} & column vector
\\
\code{Matrix<T,1,Dynamic>} & row vector
\end{tabular}
\end{center}
%
The template parameter \code{T} indicates the type of elements in the
matrix or vector.  Here, the template parameter will be instantiated
to \code{double} for constants or \code{agrad::var} for automatic
differentiation through the matrix.  


\subsection{Type Inference}

The arithmetic operations are typed, which allows inference of the
result type.  For example, multiplying two matrices produces a matrix,
multiplying a matrix by a column vector produces a column vector, and
multiplying a row matrix by a matrix produces a row matrix.  As
another example, multiplying a row vector by a column vector produces
a scalar.  

Matrix addition is treated differently.  Two matrices can be added, as
can two column vetors or two row vectors.  Adding a scalar to a matrix
(vector) is defined to add the scalar to each element of the matrix
(vector).

\subsection{Dot Products}

For example, multiplying a row vector by a column vector of the same
size produces a scalar; if they are not the same size, an exception is
raised.  If both arguments are \code{double}, Eigen matrix
multiplication is used to produce the result.  

The naive approach to dot products is to just evaluate the product and
sums.  With two $N$-vectors $x$ and $y$, the resulting expression
introduces $2N - 1$ nodes ($N$ multiplication, $N-1$ addition), one
for each operator in
\[
\left( x_1 \times y_1 \right)
+ \left( x_2 \times y_2 \right) 
+ \cdots +
\left( x_N \times y_N \right).
\]

Matrix arithmetic functions such as multiplication essentially follow
the design of the variadiac functions as discussed in
\refsection{variadic-functions}.   A specialized \code{vari} extension
is used to cut the number of nodes allocated from $2N$ to 1 and cut the
number of edges traversed during derivative propagation from $4N$ to $2N$.  

The following function definition applies to multiplying a row vector
by a column vector;  there is also a \code{dot\_product} operation
that applies to row or column vectors in either position.
%
\begin{quote}
\begin{Verbatim}
var multiply(const Matrix<var,1,Dynamic>& v1,
             const Matrix<var,Dynamic,1>& v2) {
  return var(new dot_product_vari(v1,v2));
}
\end{Verbatim}
\end{quote}
%
The following is a simplified version of the actual
\code{dot\_product\_vari} class; the library implementation includes
extra methods for other uses and is highly templated for flexiblity).
%
\begin{quote}
\begin{Verbatim}
struct dot_product_vari : public vari {
  vari** v1_;    vari** v2_;
  size_t size_;
  
  dot_product_vari(vari** v1, vari** v2, size_t size) 
  : vari(dot(v1,v2,size)), v1_(v1), v2_(v2), size_(size) { }

  dot_product_vari(const Matrix<var,1,Dynamic>& v1,
                   const Matrix<var,Dynamic,1>& v2)
  : vari(dot(v1,v2)),
    v1_(static_cast<vari**>(operator new(sizeof(vari*) * size))),
    v2_(static_cast<vari**>(operator new(sizeof(vari*) * size))),
    size_(v1.size()) {
      for (size_t n = 0; n < size_; ++n) {
        v1_[n] = v1[n].vi_;  
        v2_[n] = v2[n].vi_;
      }
  }

  void chain() {
    for (size_t n = 0; n < size_; ++n) {
      v1_[n]->adj_ += adj_ * v2_[n]->val_;
      v2_[n]->adj_ += adj_ * v1_[n]->val_;
    }
  }
};
\end{Verbatim}
\end{quote}
%
The first constructor, for arrays of \code{vari*}, will be used later
for matrix multiplication.  The second constructor, for vectors of
\code{var}, is what is used in the function definition above.  Both
the constructors compute the result using a call to an overloaded
function, neither definition of which is shown.  The arena-based
\code{operator}~\code{new} is used to allocate the member arrays
\code{v1\_} and \code{v2\_}, then the body of the constructor
populates them with the implementation pointers of the input operands.
The chain rule is also straightforward because the partial with
respect to each operand component is the corresponding component of
the other operand vector.

\subsubsection{Mixed Type Operations}

Taking the dot product of a vector with \code{double} entries and one
with \code{var} entries cannot be done directly using the \code{Eigen}
multiplication operator because it only applies to inputs with
identical entry types.  

A very naive approach would be to promote the \code{double} entries to
\code{var} and then multiply two \code{var} matrices.  This introduces
$N$ unnecessary nodes into the expression graphs and involves $N$
unnecessary gradient propagation steps to constants.  A slightly less
naive approach would be to write the loop directly, but that
is problematic for the same reason as using a loop directly for two
vectors of \code{var} entries.  Instead, \code{stan::agrad} introduces
a custom \code{var} vector times \code{double} vector dot product
function that introduces a single node into the expression graph and
only requires $N$ propagations for the dot product of $N$-vectors.


\subsection{Matrix Multiplication}

To multiply two matrices, a matrix is created for the result and each
entry is populated with the appropriate product of a row vector and
column vector.  A naive implementation for two matrices both composed
of \code{var} entries could just use Eigen's built-in matrix
multiplication, but this has the same problem as dot products, only
$N^2$-fold.

To avoid introducing unnecessary nodes into the expression graph, an
array of \code{vari*} is allocated in the autodiff memory arena and
populated with the corresponding implementation pointers from the
operand matrix rows or columns.  For the result matrix, an instance of
\code{dot\_product\_vari} is constructed using the corresponding row
and column variable implementation pointer arrays.  The resulting
storage is much more economical than if a separate dot product were
created for each entry; for instance, in multiplying a pair of $N
\times N$ matrices, separate operand storage for each result entry
would require a total of $\mathcal{O}(N^3)$ memory, whereas the
current scheme requires only $\mathcal{O}(N^2)$ memory.


\subsection{Specialized Matrix Multiplication}

The \code{stan::agrad} library provides a custom
\code{multiply\_self\_transpose} function because the memory required
can be cut in half and the speed improved compared to applying the
transposition and then the general matrix multiplication function. The
same vectors make up the original matrix's rows and the transposed
version's columns, so the same array of \code{vari**} can be reused.

Reducing memory usage often has the pleasant side effect of increasing
speed.  In the case of multiplying a matrix by its own transpose,
fewer copy operations need to be performed.  In general, memory
locality of algorithms will also be improved because of less need to
bring copies into memory.  

There is a further saving in multiplying a matrix by its own transpose
by the elimination of the explicit transposition.  Although Eigen
performs transposition with an expression template, the
\code{stan::agrad} library would make a copy.  Although copying is
rarely the bottleneck when autodiff is involved, it's not free; for
Eigen matrices or standard library vectors, it involves memory
allocation, traversal of whatever structure's being copied, and
subsequent memory deallocation.

If the matrix is lower triangular, only the non-zero portions of the
matrix need to be stored and traversed and saved in the expression
graph.  Multiplying a lower-triangular matrix by its own transposition
is common in multivariate statistics, where it is used to reconstruct
a positive definite matrix $\Omega$, such as a correlation,
covariance, or precision matrix, from its Cholesky factor, $\Omega =
L\,L^{\top}$.  So \code{stan::agrad} provides a built-in
\code{multiply\_self\_transpose\_lower\_tri} function.




\section{Linear Algebra}

Basic matrix arithmetic operations just apply standard arithmetic
operations to matrix elements.  Linear algebra operations are much
more complicated in their internal structure.  Two excellent resources
for results relating to derivatives of matrices are
\cite{PetersenPedersen:2008,MagnusNeudecker:2007}, with
\cite{Giles:2008} providing a translation to automatic differentiation
terms.

\subsection{Log Determinants}

A common operation to calculate is the logarithm of the absolute
determinant of a matrix; it shows up in change of variables problems
and as the normalizing term of the multivariate normal density.  If
$x$ is an $N \times N$ matrix, then the partials of the log absolute
determinant are given by
\[
\frac{\partial}{\partial x} \log | \mbox{det}(x) |
= \left( x^{-1} \right)^{\top}.
\]
On an element by element basis, this reduces to
\[
\frac{\partial}{\partial x_{m,n}} \log | \mbox{det}(x) |
= \left( \left( x^{-1} \right)^{\top}\right)_{m,n}.
\]

\subsubsection{Precomputed Gradients Implementation}

In some cases, such as log determinants, it is easier to compute at
the same time as the function value.  The following utility variable
implementation will be used to store vector and matrix derivatives
with precomputed gradients.
%
\begin{quote}
\begin{Verbatim}
struct precomputed_gradients_vari : public vari {
  const size_t size_;
  vari** varis_;
  double* gradients_;

  precomputed_gradients_vari(double val, size_t size, 
                             vari** varis, double* gradients)
        : vari(val), size_(size),
          varis_(varis), gradients_(gradients) { }

  void chain() {
    for (size_t i = 0; i < size_; ++i) 
      varis_[i]->adj_ += adj_ * gradients_[i];
  }
};
\end{Verbatim}
\end{quote}
%
In addition to the value and adjoint stored by the parent class
(\code{vari}), this class adds a parallel array of operands and
gradients along with their size.  The chain rule just propagates the
gradients to the operands via the chain rule.

The following member template function of \code{stack\_alloc} allows
arrays of any type to be allocated within the memory arena for
automatic differentiation.
%
\begin{quote}
\begin{Verbatim}
template <typename T>
inline T* alloc_array(size_t n) {
  return static_cast<T*>(alloc(n * sizeof(T)));
}
\end{Verbatim}
\end{quote}
%
The log gradient function uses this allocation method to populate a
precomputed gradients variable implementation.


\subsubsection{Log Determinant Implementation}

The implementation of derivatives for the log determinant involves
performing a Householder QR decomposition of the double values of the
matrix, at which point the log absolute determinant and inverse can
easily be extracted and manipulated into a form to be used

\begin{quote}
\begin{Verbatim}
template <int R, int C>
inline var 
log_determinant(const Eigen::Matrix<var,R,C>& m) {
  Matrix<double,R,C> m_d(m.rows(),m.cols());
  for (int i = 0; i < m.size(); ++i)
    m_d(i) = m(i).val();

  Eigen::FullPivHouseholderQR<Matrix<double,R,C> > hh
    = m_d.fullPivHouseholderQr();

  double val = hh.logAbsDeterminant();

  vari** operands = ChainableStack::memalloc_
                  .alloc_array<vari*>(m.size());
  for (int i = 0; i < m.size(); ++i)
    operands[i] = m(i).vi_;

  Matrix<double,R,C> m_inv_transpose 
    = hh.inverse().transpose();
  double* gradients = ChainableStack::memalloc_
                        .alloc_array<double>(m.size());
  for (int i = 0; i < m.size(); ++i)
    gradients[i] = m_inv_transpose(i);

  return var(new precomputed_gradients_vari(
                   val,m.size(),operands,gradients));
}
\end{Verbatim}
\end{quote}
%
First, the values of the \code{var} matrix are extracted and used to
set the values in the \code{double} matrix \code{m\_d}.  Then the
decomposition is performed using Eigen's \code{FullPivHouseHholderQR}
class, which according to its documentation, which provides very good
numerical stability \citep[Section~5.1]{GolubVanLoan:96} Next, the value
of the log absolute determinant is extracted from the decomposition
class.  Then the operands are set to the argument matrix's variable
implementations to be used in the reverse pass of automatic
differentiation.  Next, the inverse is transposed and used to populate
the double array \code{gradients}.  The final step is to allocate (on
the arena) a new precomputed gradients variable implementation (see
the previous section for definitions); it is then wrap it in a
\code{var} for return.
 
The memory for both the gradients and the operands for the precomputed
gradients implementation is allocated using the arena-based allocator
\code{memalloc\_}.  The precomputed gradients structure cannot store
standard vectors or Eigen vectors directly because they would not be
properly deleted.%
%
\footnote{The \code{stan::agrad} memory manager provides an additional
  stack to store pointers to \code{chainable} implementations that
  need to have their destructors called.}

The forward pass requires the value of the log determinant.  It would
be possible to be lazy and reconstruct the input matrix from the
operands and then calculate its inverse in the reverse pass.  This
would save a matrix of double values in storage, but would require an
additional QR decomposition.

Critically for speed, all matrix calculations are carried out directly
on \code{double} values rather than on \code{var} values.  The primary
motivation is to reduce the size of the expression graph and
subsequent time spent propagating derivatives in the reverse pass.  A
pleasant side effect is that the matrix operations are carried out
with all of Eigen's optimizations in effect; many of these are only
possible with the memory locality provided by \code{double} values.


\subsubsection{Specialized Log Determinant Implementation}

The \code{stan::agrad} library provides a specialized log determinant
implementation for symmetric, positive-definite matrices, which are
commonly used as metrics for geometrical applications or as
correlation, covariance or precision matrices for statistical
applications.  Symmetric, positive-definite matrices can be Cholesky factored
using the $\mbox{LDL}^{top}$ 
\citep[Chapter~4]{GolubVanLoan:96}, which Eigen provides through its
\code{LDLT} class.  Like the QR decomposition, after the decomposition
is performed, extracting the log determinant and inverse is efficient.



\section{Differential Equation Solver}

The \code{stan::agrad} library provides a differential equation solver
and allows derivatives of solutions to be calculated with respect to
parameters and/or initial values.  The system state is coded as a
\code{std::vector} and the system of equations coded as a functor.

\subsection{Systems of Ordinary Differential Equations}

Systems of differential equations describe the evolution of a
multidimensional system over time from a given starting point.  

The state of an $N$-dimensional system at time $t$ will be represented
by a vector $y(t) \in \reals^N$. The change in the system is
determined by a function $f(y,\theta,t)$ that returns the change in
state given current position $y$, parameter vector $\theta \in
\reals^K$, and time $t \in \reals$,
\[
\frac{\totald}{\totald t} y = f(y,\theta,t).
\]
The notation $f_n(y,\theta,t)$ will be used for the $n$-th component
of $f(y,\theta,t)$, so that the change in $y_n$ can be given by
\[
\frac{\totald}{\totald t} y_n = f_n(y,\theta,t).
\]

Given an initial position 
\[
\xi = y(0) \in \reals^N
\]
at initial time $t = 0$, parameter values $\theta$, and system
function $f$, it is possible to solve for state position $y(t)$ at
other times $t$ using numerical methods.%
%
\footnote{It is technically possible to solve systems backward in time
  for $t < t_0$, but \code{stan::agrad} is currently restricted to
  forward solutions; signs can be reversed to code backward time
  evolution.}
%
The \code{stan::agrad} library also supports initial positions given
at times other than 0, which can be reduced to systems with initial
time 0 with offsets.  \cite[Chapter~17]{PressEtAl:2007} provides an
overview of ODE solvers. 


\subsubsection{Example: Simple Harmonic Oscillator}

The simple harmonic osillator in two dimensions ($N = 2$) with a
single parameter $\theta$ is given by the state equations
\[
\frac{\totald}{\totald t} y_1 = y_2
\ \ \ \ \mbox{and} \ \ \ \
\frac{\totald}{\totald t} y_2 = -\theta y_1.
\]
%
which can be written as a state function as
\[
f(y,\theta,t) = [y_2 \ \ \ -\theta y_1].
\]


% \subsubsection{Example: Soil Carbon Respiration}

% For example, consider a model of carbon respiration that assumes two
% pools of carbon, a labile (fast changing) pool and a recalcitrant
% (slow changing) pool.  The pools can be representinged with a
% two-dimensional state $C(t) = \left[ C_1(t) \ \ \ C_2(t) \right]$
% where $C_k(t)$ is the concentration of carbon in pool $k$ at time $t$.
% A simple model accounts for decomposition ($\kappa$) and transfer
% ($\alpha$) of carbon,
% %
% \[
% \frac{d}{dt} C_1(t) = -\kappa_1 \, C_1(t) + \alpha_{2,1} \, \kappa_2 \, C_2(t)
% \]
% %
% \[
% \frac{d}{dt} C_2(t) = -\kappa_2 \, C_2(t) + \alpha_{1,2} \, \kappa_1 \, C_1(t)
% \]
% %
% Given an intial state $C(0)$ and values for parameters
% $\kappa_1,\kappa_2,\alpha_{2,1},\alpha_{1,2}$, the system can be
% solved through numerical integration to produce values $C(t)$ for a
% specified sequence of $t$. 


\subsection{Sensitivities of Solution to Inputs}

Solutions of differential equations are often evaluated for their
sensitivity to variation in parameters, i.e.,
\[
\frac{\partial}{\partial \theta} y(t),
\]
where $\theta$ is a vector of parameters, initial state, or both.
This is useful for applications in optimization or parameter
estimation and in reporting on (co)variation with respect to parameters.


\subsection{Differentiating the Integrator}

One way to compute sensitivities is to automatically differentiate the
numerical integrator.  This can be done with \code{stan::agrad} with a
suitably templated integrator, such as Boost's odeint
\citep{AhnertMulansky:2014}. For example, \citep{WeberEtAl:2014} used
the Dormand Prince integrator from odeint, a fifth-order Runge-Kutta
method.  Because odeint provides only a single template parameter, all
input variables (time, initial state, system parameters) were promoted
to \code{var}.

But this is inadvisable for two reasons.  First, it generates a large
expression graph which consumes both memory and time during automatic
differentiation.  This problem is exacerbated by the overpromotion
required by limited templating.  Second, there is no way to control
the error in the sensitivity calculations.


\subsection{Coupled System}

Both of the problems faced when attempting to differentiate the
integrator can be solved by instead creating a coupled system by
adding state variables for the sensitivities to the original system.
The partial derivatives are then given by the solutions for the
sensitivities.  

For each state variable $n$ and each parameter or initial state
$\alpha_m$ for which sensitivities are required, a newly defined state
variable
\[
z_{n,m} = \frac{\partial}{\partial \alpha_m} y_m
\]
is added to the system.  This produces coupled systems of the
following sizes, based on wheter derivatives are required with respect
to the initial state, system parameters, or both.
%
\begin{center}
\begin{tabular}{cc|c}
\multicolumn{2}{c|}{\it Sensitivities} &
\\
{\it Initial State} & {\it Parameters} & {\it Coupled Size}
\\ \hline
+ & - & $N \times (N + 1)$
\\
- & + & $N \times (K + 1)$ 
\\
+ & + & $N \times (N + K + 1)$
\end{tabular}
\end{center} 

With the coupled system, sensitivities are calculated by the
integrator step by step and thus can be controlled for error.  The
resulting sensitivities at solution times can then be used to
construct an autodiff variable implementation of the output expression
with respect to its inputs (initial state and/or parameters).

The \code{stan::agrad} library currently uses the Dormand-Prince
integrator implementation from Boost's odeint library
\citep{AhnertMulansky:2014}, but the design is modular, with
dependencies only on solving integration problems using \code{double}
values.   So other integrators such as Sundials (???) and lsoda (???)
may be useful, lsoda in particular because it will automatically
handle stiff systems.


\subsection{Differentiating an ODE Solution with Respect to a Parameter}

The sensitivity for state dimension $n$ with respect to parameter $k$
will be
\[
z = \frac{\partial}{\partial \theta} y,
\]
or componentwise, as
\[
z_{n,m} = \frac{\partial}{\partial \theta_m} y_n.
\]
%
The time derivatives of $z$ needed to complete the coupled system can
be given via a function $h$, defined and derived componentwise as
%
\begin{eqnarray*}
h_{n,m}(y,z,\theta,t)
& = & \frac{\totald}{\totald t} z_{n,m}.
\\[3pt]
& = &
\frac{\totald}{\totald t} \frac{\totald}{\totald \theta_m} y_n
\\[3pt]
& = &
\frac{\totald}{\totald \theta_m} \frac{\totald}{\totald t} y_n
\\[3pt]
& = & 
\frac{\totald}{\totald \theta_m} f_n(y,\theta)
\\[3pt]
& = & 
\frac{\partial}{\partial \theta_m} f_n(y,\theta)
+ \sum_{j = 1}^N \left( \frac{\partial}{\partial \theta_m} y_j \right) \, 
         \frac{\partial}{\partial y_j} f_n(y,\alpha)
\\[3pt]
& = & 
\frac{\partial}{\partial \theta_k} f_n(y,\theta)
+ \sum_{j=1}^N z_{j,m} \frac{\partial}{\partial y_j} f_n(y,\theta).
\end{eqnarray*}
%
The sensitivity of a parameter is the derivative of the state of the
system with respect to that parameter, with all other parameters held
constant (but not the states).  Thus the sensitivities act as partial
derivatives with respect to the parameters but total derivatives with
respect to the states, because of the need to take into account the
change in solution as parameters change.

The coupled system will also need new initial states $\chi_{n,m}$ for
the sensitivities $z_{n,m}$, all of which work out to be zero, because
\[
\chi_{n,m} = \frac{\partial}{\partial \theta_m} \xi_n = 0.
\]

The final system that couples the original state with sensitivities
with respect to parameters has state $(y,z)$, initial conditions
$\xi,\chi$, and system function $(f,h)$.

\subsection{Differentiating an ODE Solution with Respect to the Initial State}

The next form of coupling will be of initial states, with new state
variables
\[
w = \frac{\partial}{\partial \xi} y,
\]
which works out componentwise to
\[
w_{n,k} = \frac{\partial}{\partial \xi_{k}} y_n.
\]
Sensitivities can be worked out in this case by definining a new
system with state variables offset by the initial condition,
\[
u = y - \xi.
\]
This defines a new system function $g$ with the original parameters
$\theta$ and original initial state $\xi$ now both treated as
parameters,
\[
g(u,(\theta,\xi)) = f(u + \xi, \theta).
\]
The new initial state is a zero vector by construction.  The
derivatives are now with respect to the parameters of the revised
system with state $u$, system function $g$, and parameters
$\theta,\xi$, and work out to
%
\begin{eqnarray*}
\frac{\totald}{\totald t} w_{n,k}
& = & \frac{\totald}{\totald t} \frac{\totald}{\totald \xi_{k}} y_n
\\[3pt]
& = & \frac{\totald}{\totald \xi_k} \frac{\totald}{\totald t} y_n
\\[3pt]
& = & \frac{\totald}{\totald \xi_k} f_n(y,\theta)
\\[3pt]
& = & \frac{\totald}{\totald \xi_k} f_n(u + \xi, \theta)
\\[3pt]
& = & \frac{\partial}{\partial \xi_k} f_n(u + \xi, \theta)
\\[3pt]
& & 
{ } + \sum_{j=1}^N 
        \left( \frac{\partial}{\partial \xi_k} u_j \right)
           \frac{\partial}{\partial u_j} f_n(u + \xi, \theta)
\\
& & 
 { } + \sum_{j=1}^N
         \left( \frac{\partial}{\partial \xi_k} \xi_j \right)
         \frac{\partial}{\partial \xi_j} f_n(u + \xi, \theta)
\\[6pt]
& = & 
\frac{\partial}{\partial \xi_k} f_n(u + \xi, \theta)
\\
& & { } + \sum_{j=1}^N \left( u_j + \frac{\partial}{\partial \xi_k} \xi_j
\right)
\frac{\partial}{\partial y_j} f_n(y,\theta).
\end{eqnarray*}
%
The derivative $\partial \xi_j / \partial \xi_k$ on the last line is
equal to 1 if $j = k$ and equal to 0 otherwise.


\subsection{Computing Sensitivities with Nested Automatic Differentiation}

In order to add sensitivities with respect to paramters and/or initial
states to the system, Jacobians of the system function $f$ are
required with respect to the system state and parameters (where the
parameters may include the initial states, as shown in the last
section).  This is problematic because the derivative stack and memory
allocation are static class variables and hence global.

To get around this problem, \code{stan::agrad} allows nested
derivative evaluations.  When the system derivatives are required to
solve the system of ODEs, the current stack location is recorded,
autodiff of the system takes place on the top of the stack, and then
derivative propagation stops at the recorded stack location.  This
allows arbitrary reverse-mode automatic differentiation to be nested
inside other reverse-mode calculations.  The top of the stack can even
be reused for Jacobian calculations without rebuilding an expression
graph for the system being differentiated.

\subsection{Putting Results Back Together}

With nested calculations of the system Jacobian with respect to state
variables along with Jacobians with respect to parameters and/or
initial states, the derivatives of the system results are easy to put
back together into new \code{vari} instances.  

When the numerical integrator solves the coupled system, the solution
is for the original state variables $y$ along with sensitivities
$\frac{\totald}{\totald \theta} y$ and/or $\frac{\totald}{\totald \xi}
y$.  These are all double-precision floating point values---the nested
derivatives are all on the inside of the coupled system function.

Given a number of solution times requested, $t_1,\ldots,t_J$, These
state solutions and sensitivity solutions are used to create a
\code{vari} instance for each $y_n(t_j)$.  Each of these variables is
then connected via its sensitivity to each of the input parameters
and/or initial states.  This requires storing the sensitivities as
part of the result variables in a general (not ODE specific)
precomputed-gradients \code{vari} structure.

\subsection{System Function}

Given an initial condition and a set of requested solution times, the
\code{stan::agrad} library can integrate an ODE defined in terms of a
system function.  The system function must be able to be instantiated
by the following signature (where \code{vector} is \code{std::vector})
%
\begin{quote}
\begin{Verbatim}
vector<var>
operator()(double t, 
           const vector<double>& y,
           const vector<var>& theta) const;
\end{Verbatim}
\end{quote}
%
The return value is the vector of time derivatives evaluated at time
\code{t} and system state \code{y}, given parameters \code{theta},
continuous and integer data \code{x} and \code{x\_int}, and an output
stream \code{o} for messages.  The function must be constant.

The simple harmonic oscillator could be implemented as the following
structure.
%
\begin{quote}
\begin{Verbatim}
struct sho {
  template <typename T>
  vector<T> operator()(double t, 
                       const vector<double>& y
                       const vector<var>& theta) const
    vector<T> dy_dt(2);
    dy_dt[0] = y[1];
    dy_dt[1] = -theta[0] * y[2];
    return dy_dt;
  }
};
\end{Verbatim}
\end{quote}


\subsection{Integration Function}

The function provided by \code{stan::agrad} to compute solutions to
ODEs and support sensitivity calculations through autodiff has the
following signature.
%
\begin{quote}
\begin{Verbatim}
template <typename F, typename T1, typename T2>
vector<vector<typename promote_args<T1,T2>::type> >
integrate_ode(const F& f,
              const std::vector<T1> y0,
              const double t0,
              const std::vector<double>& ts,
              const std::vector<T2>& theta);
\end{Verbatim}
\end{quote}
%
The argument \code{f} is the system function, and it must be
implemented with enough generality to allow autodiff with respect to
the parameters and/or initial state.  The variable \code{y0} is the
initial state and \code{t0} is the initial time (which may be
different than 0); its scalar type parameter \code{T1} may be either
\code{double} or \code{var}, with \code{var} being used to autodiff
with respect to the initial state.  The vector \code{ts} is a set of
solution times requested and must have elements greater than \code{t0}
arranged in strictly ascending order.  The vector \code{theta} is for
system parameters; its scalar type parameter \code{T2} can be either
\code{double} or \code{var}, with \code{var} used to autodiff with
respect to parameters.  There are two additional arguments, \code{x}
and \code{x\_int}, used for real and double-valued data respectively.
These are provided so that data does not need to be either hard coded
in the system or promoted to \code{var} (which would add unnecessary
components to the expression graph, increasing time and memory used).
Finally, an output stream pointer can be provided for messages.

Given the harmonic oscillator class \code{sho}, and initial values
$(-1,0)$, the solutions for the harmonic oscillator at times $1{:}10$
with parameter $\theta = 0.35$, initial state $y(0) = (-1,0)$, can be
obtained in the usual way.
%
\begin{quote}
\begin{Verbatim}
double t0 = 0.0;   
vector<double> ts;  
for (int t = 1; t <= 10; ++t) ts.push_back(t);
var theta = 0.35;
vector<var> y0(2);  y[0] = 0;  y[1] = -1.0;
vector<var> ys 
  = integrate_ode(sho(), y0, t0, ts, theta);
for (int i = 0; i < 2; ++i) {
  if (i > 0) set_zero_all_adjoints();
  y.grad();
  for (int n = 0; n <= ys.size(); ++n) 
    printf("y(%2d,%2d) = %5.2f\n", ts[n], i, ts[n][i])
    printf("sens: theta=%5.2f y0(0)=%5.2f y0(1)=%5.2f\n",
           theta.adj(), y0[0].adj(), y0[1].adj());
  }
}
\end{Verbatim}
\end{quote}





\section{Probability Functions}


\section{Software Process}

\subsection{Version Control}

The source for \code{agrad::rev} is managed with GitHub.  

\subsection{Code Review via Pull Requests}
ve
Every change to the development branch of the code is formulated as a
pull request for easy comparison with the existing code base.  

\subsection{Unit Tests}

Each pull request is unit tested against a large suite of unit tests.
The continuous integration is managed through Jenkins, with testing
for Mac and Windows.  Unit tests are implemented with the Google unit
test framework for C++ \citep{GoogleTest:2014}.






\clearpage
\appendix

\section{Derivatives}\label{derivative-definitions.section}

\subsection{Derivatives}

The derivative of a unary function $f:\reals \rightarrow \reals$ is
also a unary function, conventionally written as $f'$.  The postfixed prime is
the derivative operator, which maps unary functions to unary
functions, $\cdot' : \left(\reals \rightarrow \reals\right)
\rightarrow \left(\reals \rightarrow \reals\right)$.  The notation
$\frac{\partial}{\partial x} f(x)$ can be thought of as a shorthand
for $\left( \lambda x. f(x) \right)'$, which uses the lambda-calculus
notation for functions. The $\partial x$ in the denominator can be
thought of as binding the free occurrence of $x$ in $f(x)$.  The
notation
\[
\left. \frac{\partial}{\partial x} f(x) \right|_{x = u}
\]
can also be written in terms of the derivative operator as $\left(
  \lambda x. f(x) \right)'(u),$ or in this case, simply $f'(u)$.

\subsection{Gradients}

The gradient operator $\nabla:(\reals^N \rightarrow \reals)\rightarrow
(\reals^N \rightarrow \reals^N)$ generalizes derivatives to multiple
input dimensions.  The operator is defined so that if
$f : \reals^N \rightarrow \reals$, then $(\nabla f):\reals^N \rightarrow
\reals^N$ is a function mapping a point $x \in \reals^N$ to the
derivative vector of $f$ evaluated at $x$, namely
%
\[
(\nabla f)(x) 
= 
\left[
  \left. 
    \frac{\partial}{\partial u_1} f(u_1,x_2,\ldots,x_N)
  \right|_{u_1 = x_1}
  \ \ \cdots \ \ 
  \left. 
    \frac{\partial}{\partial u_N}  f(x_1,\ldots,x_{N-1},u_N)
   \right|_{u_N = x_N}
\right]
\]
%
Here new variables $u_n$ are introduced as bound variables to avoid
any confusion arising from using $x_n$ both as a component of the
evaluation point $x$ and as a bound variable for differentiation.
Within the $n$-th partial evaluation in the result, the variables
$x_1,\ldots,x_{n-1}$ and $x_{n+1},\ldots,x_N$ are constants.  As with
derivative symbols, the notation $\nabla_{\!x} \, f(x)$ uses a
subscript to implicitly indicate binding of the $x$ in $f(x)$ and is
defined to be equivalent to $\nabla(\lambda x. f(x))$.

\subsection{Jacobians}

The Jacobian of a function $g : \reals^N \rightarrow \reals^M$ is 
the $M \times N$ matrix formed by stacking the transposed gradients of $g$ projected down to
each of the $M$ result dimensions,
%
\[
J(g)(x) = 
\left[
\begin{array}{c}
(\nabla g_1)(x)^{\top}
\\[2pt]
\vdots
\\[2pt]
(\nabla g_M)(x)^{\top}
\end{array}
\right]
\]
%
where $g_m(x)$ is defined to be $g(x)_m$, the $m$-th component of $g(x)$.


\subsection{Matrix Derivatives}

If $f:\reals^{M \times N} \rightarrow \reals$ is a function on $M
\times N$ matrices, the derivative of $f$ with respect to $x$ is
defined so that 
\[
\left( \frac{\partial}{\partial x} f(x) \right)_{i,j}
=
\frac{\partial}{\partial x_{i,j}} f(x).
\]

\subsection{Chain Rule}

The univariate form of the chain rule ensures that if $f,g:\reals
\rightarrow \reals$ and $x \in \reals$, then 
\[
(f \circ g)'(x) = f'(g(x)) \, g'(x).
\]
The multivariate form for partial derivatives ensures that if
$f:\reals^N \rightarrow \reals$, $g:\reals^M \rightarrow \reals^N$,
and $x \in \reals^M$, then
\[
\nabla^{\top}\!(f \circ g) (x)
= 
  \left( \nabla^{\top} f \right)\!(g(x))
  \times
  J(g)
\]
where the product is of the transposed gradient of $f$ evaluated at
$g(x)$ (a $1 \times N$ row vector) and the Jacobian of $g$ (an $N
\times M$ matrix), producing the transpose of the gradient of $f \circ
g$ (a $1 \times M$ vector).



\clearpage
\nocite{Hogan:2014}
\nocite{Bell:2012}
\nocite{VandevoordeJosuttis:2002}
\nocite{Giles:2008}
\nocite{Gay:2005}
\nocite{GriewankWalther:2008}
\nocite{Fog:2014}
\nocite{Hogan:2014}
\nocite{Neal:2003}

\bibliographystyle{apalike}
\bibliography{../../bibtex/all}



\end{document}